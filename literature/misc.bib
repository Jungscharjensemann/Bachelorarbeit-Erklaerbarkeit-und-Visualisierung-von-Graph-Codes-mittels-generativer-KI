@misc{gpt3-paper,
      title={Language Models are Few-Shot Learners},
      author={{BROWN, T. B.}, and others},
      year={2020},
      url={https://arxiv.org/abs/2005.14165}
}

@misc{google-attention,
      title={Attention Is All You Need},
      author={{VASWANI, A.} and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      url={https://arxiv.org/abs/1706.03762}
}

@misc{zero-shot-reasoners,
      title={Large Language Models are Zero-Shot Reasoners},
      author={{KOJIMA, T.} and {GU, S. S.} and {REID, M.} and {MATSUO, Y.} and {IWASAWA, Y.}},
      year={2023},
      url={https://arxiv.org/abs/2205.11916}
}

@misc{google-cot,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
      author={{WEI, J.} and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      url={https://arxiv.org/abs/2201.11903}
}

@misc{goodfellow-gan,
      title={Generative Adversarial Networks},
      author={{GOODFELLOW, I. J.} and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      url={https://arxiv.org/abs/1406.2661}
}
